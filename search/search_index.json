{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TransQuest: Translation Quality Estimation with Cross-lingual Transformers The goal of quality estimation (QE) is to evaluate the quality of a translation without having access to a reference translation. High-accuracy QE that can be easily deployed for a number of language pairs is the missing piece in many commercial translation workflows as they have numerous potential uses. They can be employed to select the best translation when several translation engines are available or can inform the end user about the reliability of automatically translated content. In addition, QE systems can be used to decide whether a translation can be published as it is in a given context, or whether it requires human post-editing before publishing or translation from scratch by a human. The quality estimation can be done at different levels: document level, sentence level and word level. With TransQuest, we have opensourced our research in translation quality estimation which also won the sentence-level direct assessment quality estimation shared task in WMT 2020 . TransQuest outperforms current open-source quality estimation frameworks such as OpenKiwi and DeepQuest . Features Sentence-level translation quality estimation on both aspects: predicting post editing efforts and direct assessment. Word-level translation quality estimation capable of predicting quality of source words, target words and target gaps. Perform significantly better than current state-of-the-art quality estimation methods like DeepQuest and OpenKiwi in all the languages experimented. Pre-trained quality estimation models for fifteen language pairs. Table of Contents Installation - Install TransQuest locally using pip. Architectures - Checkout the architectures implemented in TransQuest Sentence-level Architectures - We have released two architectures; MonoTransQuest and SiameseTransQuest to perform sentence level quality estimation. Word-level Architecture - We have released MicroTransQuest to perform word level quality estimation. Examples - We have provided several examples on how to use TransQuest in recent WMT quality estimation shared tasks. Sentence-level Examples Word-level Examples Pre-trained Models - We have provided pretrained quality estimation models for fifteen language pairs covering both sentence-level and word-level Sentence-level Models Word-level Models Contact - Contact us for any issues with TransQuest Resources Research Seminar done on 1st of October 2020 in RGCL and the slides . Citations If you are using the package, please consider citing this paper which is accepted to COLING 2020 1 2 3 4 5 6 @InProceedings { transquest:2020a, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest: Translation Quality Estimation with Cross-lingual Transformers } , booktitle = { Proceedings of the 28th International Conference on Computational Linguistics } , year = { 2020 } } If you are using the task specific fine tuning, please consider citing this which is accepted to WMT 2020 at EMNLP 2020. 1 2 3 4 5 6 @InProceedings { transquest:2020b, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest at WMT2020: Sentence-Level Direct Assessment } , booktitle = { Proceedings of the Fifth Conference on Machine Translation } , year = { 2020 } }","title":"Home"},{"location":"#transquest-translation-quality-estimation-with-cross-lingual-transformers","text":"The goal of quality estimation (QE) is to evaluate the quality of a translation without having access to a reference translation. High-accuracy QE that can be easily deployed for a number of language pairs is the missing piece in many commercial translation workflows as they have numerous potential uses. They can be employed to select the best translation when several translation engines are available or can inform the end user about the reliability of automatically translated content. In addition, QE systems can be used to decide whether a translation can be published as it is in a given context, or whether it requires human post-editing before publishing or translation from scratch by a human. The quality estimation can be done at different levels: document level, sentence level and word level. With TransQuest, we have opensourced our research in translation quality estimation which also won the sentence-level direct assessment quality estimation shared task in WMT 2020 . TransQuest outperforms current open-source quality estimation frameworks such as OpenKiwi and DeepQuest .","title":"TransQuest: Translation Quality Estimation with Cross-lingual Transformers"},{"location":"#features","text":"Sentence-level translation quality estimation on both aspects: predicting post editing efforts and direct assessment. Word-level translation quality estimation capable of predicting quality of source words, target words and target gaps. Perform significantly better than current state-of-the-art quality estimation methods like DeepQuest and OpenKiwi in all the languages experimented. Pre-trained quality estimation models for fifteen language pairs.","title":"Features"},{"location":"#table-of-contents","text":"Installation - Install TransQuest locally using pip. Architectures - Checkout the architectures implemented in TransQuest Sentence-level Architectures - We have released two architectures; MonoTransQuest and SiameseTransQuest to perform sentence level quality estimation. Word-level Architecture - We have released MicroTransQuest to perform word level quality estimation. Examples - We have provided several examples on how to use TransQuest in recent WMT quality estimation shared tasks. Sentence-level Examples Word-level Examples Pre-trained Models - We have provided pretrained quality estimation models for fifteen language pairs covering both sentence-level and word-level Sentence-level Models Word-level Models Contact - Contact us for any issues with TransQuest","title":"Table of Contents"},{"location":"#resources","text":"Research Seminar done on 1st of October 2020 in RGCL and the slides .","title":"Resources"},{"location":"#citations","text":"If you are using the package, please consider citing this paper which is accepted to COLING 2020 1 2 3 4 5 6 @InProceedings { transquest:2020a, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest: Translation Quality Estimation with Cross-lingual Transformers } , booktitle = { Proceedings of the 28th International Conference on Computational Linguistics } , year = { 2020 } } If you are using the task specific fine tuning, please consider citing this which is accepted to WMT 2020 at EMNLP 2020. 1 2 3 4 5 6 @InProceedings { transquest:2020b, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest at WMT2020: Sentence-Level Direct Assessment } , booktitle = { Proceedings of the Fifth Conference on Machine Translation } , year = { 2020 } }","title":"Citations"},{"location":"contact/","text":"Contact If you have any trouble using TransQuest or want to contribute to the project, please open a GitHub issue, or drop an email to Tharindu Ranasinghe . Citations If you are using the package, please consider citing this paper which is accepted to COLING 2020 1 2 3 4 5 6 @InProceedings { transquest:2020a, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest: Translation Quality Estimation with Cross-lingual Transformers } , booktitle = { Proceedings of the 28th International Conference on Computational Linguistics } , year = { 2020 } } If you are using the task specific fine tuning, please consider citing this which is accepted to WMT 2020 at EMNLP 2020. 1 2 3 4 5 6 @InProceedings { transquest:2020b, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest at WMT2020: Sentence-Level Direct Assessment } , booktitle = { Proceedings of the Fifth Conference on Machine Translation } , year = { 2020 } }","title":"Contact"},{"location":"contact/#contact","text":"If you have any trouble using TransQuest or want to contribute to the project, please open a GitHub issue, or drop an email to Tharindu Ranasinghe .","title":"Contact"},{"location":"contact/#citations","text":"If you are using the package, please consider citing this paper which is accepted to COLING 2020 1 2 3 4 5 6 @InProceedings { transquest:2020a, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest: Translation Quality Estimation with Cross-lingual Transformers } , booktitle = { Proceedings of the 28th International Conference on Computational Linguistics } , year = { 2020 } } If you are using the task specific fine tuning, please consider citing this which is accepted to WMT 2020 at EMNLP 2020. 1 2 3 4 5 6 @InProceedings { transquest:2020b, author = { Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan } , title = { TransQuest at WMT2020: Sentence-Level Direct Assessment } , booktitle = { Proceedings of the Fifth Conference on Machine Translation } , year = { 2020 } }","title":"Citations"},{"location":"install/","text":"Installation You first need to install PyTorch. The recommended PyTorch version is 1.8. Please refer to PyTorch installation page regarding the specific install command for your platform. When PyTorch has been installed, you can install TransQuest from source or from pip. Note If you are training models, we highly recommend using a GPU. We used a NVIDIA TESLA K80 GPU to train the models. From pip 1 pip install transquest From Source 1 2 3 git clone https://github.com/TharinduDR/TransQuest.git cd TransQuest pip install -r requirements.txt Tip Now that you have installed TransQuest, it is time to check our architectures in sentence-level and word-level.","title":"Install"},{"location":"install/#installation","text":"You first need to install PyTorch. The recommended PyTorch version is 1.8. Please refer to PyTorch installation page regarding the specific install command for your platform. When PyTorch has been installed, you can install TransQuest from source or from pip. Note If you are training models, we highly recommend using a GPU. We used a NVIDIA TESLA K80 GPU to train the models.","title":"Installation"},{"location":"install/#from-pip","text":"1 pip install transquest","title":"From pip"},{"location":"install/#from-source","text":"1 2 3 git clone https://github.com/TharinduDR/TransQuest.git cd TransQuest pip install -r requirements.txt Tip Now that you have installed TransQuest, it is time to check our architectures in sentence-level and word-level.","title":"From Source"},{"location":"architectures/sentence_level_architectures/","text":"Sentence Level TransQuest Architectures We have introduced two architectures for the sentence level QE in the TransQuest framework, both relies on the XLM-R transformer model. Data Preparation First read your data in to a pandas dataframe and format it so that it has three columns with headers text_a, text_b and labels. text_a is the source text, text_b is the target text and labels are the quality scores as in the following table. text_a text_b labels \u0db1\u0db8\u0dd4\u0dad\u0dca 1170 \u0dc3\u0dd2\u0da7 1270 \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0dbb\u0da2\u0dba \u0db4\u0dcf\u0dbd\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf\u0dca\u0daf\u0dda \u0dba\u0dd4\u0db0 \u0db1\u0dcf\u0dba\u0d9a\u0dba\u0dd2\u0db1\u0dca \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dd2. But from 1170 to 1270 the government was controlled by warlords. 0.8833 \u0dc0\u0dca\u200d\u0dba\u0d82\u0d9c\u0dba\u0dd9\u0db1\u0dca \u0d9c\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0d9a\u0dca \u0dba\u0db1\u0dd4 \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc0\u0da0\u0db1\u0dba\u0dd9\u0db1\u0dca \u0dc0\u0dd2\u0dc3\u0dca\u0dad\u0dbb \u0db1\u0ddc\u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0d91\u0d9a\u0dca \u0d85\u0dc0\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0d9a\u0dd2. A contract from the constitution is one of the occasions in which the term is not described. 0.6667 Now, you can consider following architectures to build the QE model. MonoTransQuest The first architecture proposed uses a single XLM-R transformer model. The input of this model is a concatenation of the original sentence and its translation, separated by the [SEP] token. Then the output of the [CLS] token is passed through a softmax layer to reflect the quality scores. Minimal Start for a MonoTransQuest Model Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section. 1 2 3 4 5 6 7 8 9 from transquest.algo.sentence_level.monotransquest.evaluation import pearson_corr , spearman_corr from sklearn.metrics import mean_absolute_error from transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel import torch model = MonoTransQuestModel ( \"xlmroberta\" , \"xlm-roberta-large\" , num_labels = 1 , use_cuda = torch . cuda . is_available (), args = monotransquest_config ) model . train_model ( train_df , eval_df = eval_df , pearson_corr = pearson_corr , spearman_corr = spearman_corr , mae = mean_absolute_error ) An example monotransquest_config is available here. . The best model will be saved to the path specified in the \"best_model_dir\" in monotransquest_config. Then you can load it and do the predictions like this. 1 2 3 4 5 6 7 from transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel model = MonoTransQuestModel ( \"xlmroberta\" , monotransquest_config [ \"best_model_dir\" ], num_labels = 1 , use_cuda = torch . cuda . is_available ()) predictions , raw_outputs = model . predict ([[ source , target ]]) print ( predictions ) Predictions are the predicted quality scores. SiameseTransQuest The second approach proposed in this framework relies on a Siamese architecture where we feed the original text and the translation into two separate XLM-R transformer models. Then the output of all the word embeddings goes through a mean pooling layer. After that we calculate the cosine similarity between the output of the pooling layers which reflects the quality of the translation. Minimal Start for a SiameseTransQuest Model First save your train/dev pandas dataframes to csv files in a single folder. We refer the path to that folder as \"path\" in the code below. You have to provide the indices of source, target and quality labels when reading with the QEDataReader class. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from transquest.algo.sentence_level.siamesetransquest import LoggingHandler , SentencesDataset , \\ SiameseTransQuestModel from transquest.algo.sentence_level.siamesetransquest import models , losses from transquest.algo.sentence_level.siamesetransquest.evaluation import EmbeddingSimilarityEvaluator from transquest.algo.sentence_level.siamesetransquest.readers import QEDataReader from torch.utils.data import DataLoader import math qe_reader = QEDataReader ( path , s1_col_idx = 0 , s2_col_idx = 1 , score_col_idx = 2 , normalize_scores = False , min_score = 0 , max_score = 1 , header = True ) word_embedding_model = models . Transformer ( \"xlm-roberta-large\" , max_seq_length = siamesetransquest_config [ 'max_seq_length' ]) pooling_model = models . Pooling ( word_embedding_model . get_word_embedding_dimension (), pooling_mode_mean_tokens = True , pooling_mode_cls_token = False , pooling_mode_max_tokens = False ) model = SiameseTransQuestModel ( modules = [ word_embedding_model , pooling_model ]) train_data = SentencesDataset ( qe_reader . get_examples ( 'train.tsv' ), model ) train_dataloader = DataLoader ( train_data , shuffle = True , batch_size = siamesetransquest_config [ 'train_batch_size' ]) train_loss = losses . CosineSimilarityLoss ( model = model ) eval_data = SentencesDataset ( examples = qe_reader . get_examples ( 'eval_df.tsv' ), model = model ) eval_dataloader = DataLoader ( eval_data , shuffle = False , batch_size = siamesetransquest_config [ 'train_batch_size' ]) evaluator = EmbeddingSimilarityEvaluator ( eval_dataloader ) warmup_steps = math . ceil ( len ( train_data ) * siamesetransquest_config [ \"num_train_epochs\" ] / siamese_transformer_config [ 'train_batch_size' ] * 0.1 ) model . fit ( train_objectives = [( train_dataloader , train_loss )], evaluator = evaluator , epochs = siamesetransquest_config [ 'num_train_epochs' ], evaluation_steps = 100 , optimizer_params = { 'lr' : siamesetransquest_config [ \"learning_rate\" ], 'eps' : siamesetransquest_config [ \"adam_epsilon\" ], 'correct_bias' : False }, warmup_steps = warmup_steps , output_path = siamesetransquest_config [ 'best_model_dir' ]) An example siamese_transformer_config is available here. . The best model will be saved to the path specified in the \"best_model_dir\" in siamesetransquest_config. Then you can load it and do the predictions like this. 1 2 3 4 5 6 7 test_data = SentencesDataset ( examples = qe_reader . get_examples ( \"test.tsv\" , test_file = True ), model = model ) test_dataloader = DataLoader ( test_data , shuffle = False , batch_size = 8 ) evaluator = EmbeddingSimilarityEvaluator ( test_dataloader ) model . evaluate ( evaluator , result_path = os . path . join ( siamesetransquest_config [ 'cache_dir' ], \"test_result.txt\" ), verbose = False ) You will find the predictions in the test_result.txt file in the siamesetransquest_config['cache_dir'] folder. Tip Now that you know about the architectures in TransQuest, check how we can apply it in WMT QE shared tasks here.","title":"Sentence-level"},{"location":"architectures/sentence_level_architectures/#sentence-level-transquest-architectures","text":"We have introduced two architectures for the sentence level QE in the TransQuest framework, both relies on the XLM-R transformer model.","title":"Sentence Level TransQuest Architectures"},{"location":"architectures/sentence_level_architectures/#data-preparation","text":"First read your data in to a pandas dataframe and format it so that it has three columns with headers text_a, text_b and labels. text_a is the source text, text_b is the target text and labels are the quality scores as in the following table. text_a text_b labels \u0db1\u0db8\u0dd4\u0dad\u0dca 1170 \u0dc3\u0dd2\u0da7 1270 \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0dbb\u0da2\u0dba \u0db4\u0dcf\u0dbd\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf\u0dca\u0daf\u0dda \u0dba\u0dd4\u0db0 \u0db1\u0dcf\u0dba\u0d9a\u0dba\u0dd2\u0db1\u0dca \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dd2. But from 1170 to 1270 the government was controlled by warlords. 0.8833 \u0dc0\u0dca\u200d\u0dba\u0d82\u0d9c\u0dba\u0dd9\u0db1\u0dca \u0d9c\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0d9a\u0dca \u0dba\u0db1\u0dd4 \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc0\u0da0\u0db1\u0dba\u0dd9\u0db1\u0dca \u0dc0\u0dd2\u0dc3\u0dca\u0dad\u0dbb \u0db1\u0ddc\u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0d91\u0d9a\u0dca \u0d85\u0dc0\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0d9a\u0dd2. A contract from the constitution is one of the occasions in which the term is not described. 0.6667 Now, you can consider following architectures to build the QE model.","title":"Data Preparation"},{"location":"architectures/sentence_level_architectures/#monotransquest","text":"The first architecture proposed uses a single XLM-R transformer model. The input of this model is a concatenation of the original sentence and its translation, separated by the [SEP] token. Then the output of the [CLS] token is passed through a softmax layer to reflect the quality scores.","title":"MonoTransQuest"},{"location":"architectures/sentence_level_architectures/#minimal-start-for-a-monotransquest-model","text":"Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section. 1 2 3 4 5 6 7 8 9 from transquest.algo.sentence_level.monotransquest.evaluation import pearson_corr , spearman_corr from sklearn.metrics import mean_absolute_error from transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel import torch model = MonoTransQuestModel ( \"xlmroberta\" , \"xlm-roberta-large\" , num_labels = 1 , use_cuda = torch . cuda . is_available (), args = monotransquest_config ) model . train_model ( train_df , eval_df = eval_df , pearson_corr = pearson_corr , spearman_corr = spearman_corr , mae = mean_absolute_error ) An example monotransquest_config is available here. . The best model will be saved to the path specified in the \"best_model_dir\" in monotransquest_config. Then you can load it and do the predictions like this. 1 2 3 4 5 6 7 from transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel model = MonoTransQuestModel ( \"xlmroberta\" , monotransquest_config [ \"best_model_dir\" ], num_labels = 1 , use_cuda = torch . cuda . is_available ()) predictions , raw_outputs = model . predict ([[ source , target ]]) print ( predictions ) Predictions are the predicted quality scores.","title":"Minimal Start for a MonoTransQuest Model"},{"location":"architectures/sentence_level_architectures/#siamesetransquest","text":"The second approach proposed in this framework relies on a Siamese architecture where we feed the original text and the translation into two separate XLM-R transformer models. Then the output of all the word embeddings goes through a mean pooling layer. After that we calculate the cosine similarity between the output of the pooling layers which reflects the quality of the translation.","title":"SiameseTransQuest"},{"location":"architectures/sentence_level_architectures/#minimal-start-for-a-siamesetransquest-model","text":"First save your train/dev pandas dataframes to csv files in a single folder. We refer the path to that folder as \"path\" in the code below. You have to provide the indices of source, target and quality labels when reading with the QEDataReader class. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from transquest.algo.sentence_level.siamesetransquest import LoggingHandler , SentencesDataset , \\ SiameseTransQuestModel from transquest.algo.sentence_level.siamesetransquest import models , losses from transquest.algo.sentence_level.siamesetransquest.evaluation import EmbeddingSimilarityEvaluator from transquest.algo.sentence_level.siamesetransquest.readers import QEDataReader from torch.utils.data import DataLoader import math qe_reader = QEDataReader ( path , s1_col_idx = 0 , s2_col_idx = 1 , score_col_idx = 2 , normalize_scores = False , min_score = 0 , max_score = 1 , header = True ) word_embedding_model = models . Transformer ( \"xlm-roberta-large\" , max_seq_length = siamesetransquest_config [ 'max_seq_length' ]) pooling_model = models . Pooling ( word_embedding_model . get_word_embedding_dimension (), pooling_mode_mean_tokens = True , pooling_mode_cls_token = False , pooling_mode_max_tokens = False ) model = SiameseTransQuestModel ( modules = [ word_embedding_model , pooling_model ]) train_data = SentencesDataset ( qe_reader . get_examples ( 'train.tsv' ), model ) train_dataloader = DataLoader ( train_data , shuffle = True , batch_size = siamesetransquest_config [ 'train_batch_size' ]) train_loss = losses . CosineSimilarityLoss ( model = model ) eval_data = SentencesDataset ( examples = qe_reader . get_examples ( 'eval_df.tsv' ), model = model ) eval_dataloader = DataLoader ( eval_data , shuffle = False , batch_size = siamesetransquest_config [ 'train_batch_size' ]) evaluator = EmbeddingSimilarityEvaluator ( eval_dataloader ) warmup_steps = math . ceil ( len ( train_data ) * siamesetransquest_config [ \"num_train_epochs\" ] / siamese_transformer_config [ 'train_batch_size' ] * 0.1 ) model . fit ( train_objectives = [( train_dataloader , train_loss )], evaluator = evaluator , epochs = siamesetransquest_config [ 'num_train_epochs' ], evaluation_steps = 100 , optimizer_params = { 'lr' : siamesetransquest_config [ \"learning_rate\" ], 'eps' : siamesetransquest_config [ \"adam_epsilon\" ], 'correct_bias' : False }, warmup_steps = warmup_steps , output_path = siamesetransquest_config [ 'best_model_dir' ]) An example siamese_transformer_config is available here. . The best model will be saved to the path specified in the \"best_model_dir\" in siamesetransquest_config. Then you can load it and do the predictions like this. 1 2 3 4 5 6 7 test_data = SentencesDataset ( examples = qe_reader . get_examples ( \"test.tsv\" , test_file = True ), model = model ) test_dataloader = DataLoader ( test_data , shuffle = False , batch_size = 8 ) evaluator = EmbeddingSimilarityEvaluator ( test_dataloader ) model . evaluate ( evaluator , result_path = os . path . join ( siamesetransquest_config [ 'cache_dir' ], \"test_result.txt\" ), verbose = False ) You will find the predictions in the test_result.txt file in the siamesetransquest_config['cache_dir'] folder. Tip Now that you know about the architectures in TransQuest, check how we can apply it in WMT QE shared tasks here.","title":"Minimal Start for a SiameseTransQuest Model"},{"location":"architectures/word_level_architecture/","text":"Word Level TransQuest Architecture WE have one architecture that is capable of providing word level quality estimation models; MicroTransQuest. Data Preparation Please have your data as a pandas dataframe in this format. source target source_tags target_tags 52 mg wasserfreie Lactose . 52 mg anhydrous lactose . [OK OK OK OK OK] [OK OK OK OK OK OK OK OK OK OK OK] Rom\u00e2nia sanofi-aventis Rom\u00e2nia S.R.L. Sanofi-Aventis Rom\u00e2nia S. R. L. [BAD OK OK OK] [BAD BAD OK OK OK OK OK OK OK OK OK] Please note that target_tags_column has word level quality labels for gaps in the target too. Therefore, it has 2*N+1 labels, where N is the total number of tokens in the target. For more information please have a look at WMT word level quality estimtion task. Now, you can consider MicroTransQuest to build the QE model. MicroTransQuest The input of this model is a concatenation of the original sentence and its translation, separated by the [SEP] token. As shown in the Figure target sentence contains gaps too. Then the output of the each token is passed through a softmax layer to reflect the quality scores. Minimal Start for a MonoTransQuest Model Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section. 1 2 3 4 5 from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel import torch model = MicroTransQuestModel ( \"xlmroberta\" , \"xlm-roberta-large\" , labels = [ \"OK\" , \"BAD\" ], use_cuda = torch . cuda . is_available (), args = microtransquest_config ) model . train_model ( train_df , eval_df = eval_df ) An example microtransquest_config is available here. . The best model will be saved to the path specified in the \"best_model_dir\" in microtransquest_config. Then you can load it and do the predictions like this. 1 2 3 4 5 6 from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel model = MicroTransQuestModel ( \"xlmroberta\" , microtransquest_config [ \"best_model_dir\" ], use_cuda = torch . cuda . is_available () ) sources_tags , targets_tags = model . predict ([[ source , target ]], split_on_space = True ) Tip Now that you know about the word-level architecture in TransQuest, check how we can apply it in WMT QE shared tasks here.","title":"Word-level"},{"location":"architectures/word_level_architecture/#word-level-transquest-architecture","text":"WE have one architecture that is capable of providing word level quality estimation models; MicroTransQuest.","title":"Word Level TransQuest Architecture"},{"location":"architectures/word_level_architecture/#data-preparation","text":"Please have your data as a pandas dataframe in this format. source target source_tags target_tags 52 mg wasserfreie Lactose . 52 mg anhydrous lactose . [OK OK OK OK OK] [OK OK OK OK OK OK OK OK OK OK OK] Rom\u00e2nia sanofi-aventis Rom\u00e2nia S.R.L. Sanofi-Aventis Rom\u00e2nia S. R. L. [BAD OK OK OK] [BAD BAD OK OK OK OK OK OK OK OK OK] Please note that target_tags_column has word level quality labels for gaps in the target too. Therefore, it has 2*N+1 labels, where N is the total number of tokens in the target. For more information please have a look at WMT word level quality estimtion task. Now, you can consider MicroTransQuest to build the QE model.","title":"Data Preparation"},{"location":"architectures/word_level_architecture/#microtransquest","text":"The input of this model is a concatenation of the original sentence and its translation, separated by the [SEP] token. As shown in the Figure target sentence contains gaps too. Then the output of the each token is passed through a softmax layer to reflect the quality scores.","title":"MicroTransQuest"},{"location":"architectures/word_level_architecture/#minimal-start-for-a-monotransquest-model","text":"Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section. 1 2 3 4 5 from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel import torch model = MicroTransQuestModel ( \"xlmroberta\" , \"xlm-roberta-large\" , labels = [ \"OK\" , \"BAD\" ], use_cuda = torch . cuda . is_available (), args = microtransquest_config ) model . train_model ( train_df , eval_df = eval_df ) An example microtransquest_config is available here. . The best model will be saved to the path specified in the \"best_model_dir\" in microtransquest_config. Then you can load it and do the predictions like this. 1 2 3 4 5 6 from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel model = MicroTransQuestModel ( \"xlmroberta\" , microtransquest_config [ \"best_model_dir\" ], use_cuda = torch . cuda . is_available () ) sources_tags , targets_tags = model . predict ([[ source , target ]], split_on_space = True ) Tip Now that you know about the word-level architecture in TransQuest, check how we can apply it in WMT QE shared tasks here.","title":"Minimal Start for a MonoTransQuest Model"},{"location":"examples/sentence_level_examples/","text":"Sentence Level Examples We have provided several examples on how to use TransQuest in recent WMT sentence-level quality estimation shared tasks. They are included in the repository but are not shipped with the library. Therefore, if you need to run the examples, please clone the repository. Warning Please don't use the same environment you used to install TransQuest to run the examples. Create a new environment. 1 2 3 git clone https://github.com/TharinduDR/TransQuest.git cd TransQuest pip install -r requirements.txt In the examples/sentence_level folder you will find the following tasks. WMT 2020 QE Task 1: Sentence-Level Direct Assessment The participants were predict the direct assessment of a source and a target. There were seven language-pairs released by the organisers. To run the experiments for each language please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2020.<language-pair>.<architecture> Language Pair options : ro_en (Romanian-English), ru_en (Russian-English), et_en (Estonian-English), en_zh (English-Chinese), ne_en (Nepalese-English), en_de (English-German), si_en(Sinhala-English) Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on Romanian-English with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2020.ro_en.monotransquest Results Both architectures in TransQuest outperforms OpenKiwi in all the language pairs. Furthermore, TransQuest won this task in all the language pairs. Language Pair Algorithm Pearson MAE RMSE Romanian-English MonoTransQuest 0.8982 0.3121 0.4097 SiameseTransQuest 0.8501 0.3637 0.4932 OpenKiwi 0.6845 0.7596 1.0522 Estonian-English MonoTransQuest 0.7748 0.5904 0.7321 SiameseTransQuest 0.6804 0.7047 0.9022 OpenKiwi 0.4770 0.9176 1.1382 Nepalese-English MonoTransQuest 0.7914 0.3975 0.5078 SiameseTransQuest 0.6081 0.6531 0.7950 OpenKiwi 0.3860 0.7353 0.8713 Sinhala-English MonoTransQuest 0.6525 0.4510 0.5570 SiameseTransQuest 0.5957 0.5078 0.6466 OpenKiwi 0.3737 0.7517 0.8978 Russian-English MonoTransQuest 0.7734 0.5076 0.6856 SiameseTransQuest 0.7126 0.6132 0.8531 OpenKiwi 0.5479 0.8253 1.1930 English-German MonoTransQuest 0.4669 0.6474 0.7762 SiameseTransQuest 0.3992 0.6651 0.8497 OpenKiwi 0.1455 0.6791 0.9670 English-Chinese MonoTransQuest 0.4779 0.9865 1.1338 SiameseTransQuest 0.4067 1.0389 1.1973 OpenKiwi 0.1676 0.6559 0.8503 WMT 2020 QE Task 2: Sentence-Level Post-editing Effort This task consists predicting Sentence-level HTER (Human Translation Error Rate) scores for a given source and a target. To run the experiments for each language please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2020_task2.<language-pair>.<architecture> Language Pair options : en_zh (English-Chinese), en_de (English-German) Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on English-Chinese with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2020_task2.en_zh.monotransquest Results Both architectures in TransQuest outperforms OpenKiwi in all the language pairs. Language Pair Algorithm Pearson MAE RMSE English-German MonoTransQuest 0.4994 0.1486 0.1842 SiameseTransQuest 0.4875 0.1501 0.1886 OpenKiwi 0.3916 0.1500 0.1896 English-Chinese MonoTransQuest 0.5910 0.1351 0.1681 SiameseTransQuest 0.5621 0.1411 0.1723 OpenKiwi 0.5058 0.1470 0.1814 WMT 2019 QE Task 1: Sentence-Level QE The participating systems are expected to predict the sentence-level HTER score (the percentage of edits needed to fix the translation) To run the experiments for each language, please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2019.<language-pair>.<architecture> Language Pair options : en_ru (English-Russian), en_de (English-German) Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on English-Russian with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2019.en_ru.monotransquest Results Both architectures in TransQuest outperforms QuEst++ in all the language pairs. Language Pair Algorithm Pearson English-German MonoTransQuest 0.5117 SiameseTransQuest 0.4951 QuEst++ 0.4001 English-Russian MonoTransQuest 0.7126 SiameseTransQuest 0.6432 QuEst++ 0.2601 WMT 2018 QE Task 1: Sentence-Level QE The participating systems are expected to predict the sentence-level HTER score (the percentage of edits needed to fix the translation) To run the experiments for each language, please run this command from the root directory of TransQuest. If both NMT and SMT is available for a certain language pair, specify that too. 1 python -m examples.sentence_level.wmt_2019.<language-pair>.<nmt/smt><architecture> Language Pair options : en_de (English-German) (both NMT and SMT), en_lv(English-Latvian) (both NMT and SMT), en_cs(English-Czech), de_en Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on English-Latvian NMT with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2018.en_lv.nmt.monotransquest To run the English-Czech experiments with MonoTransQuest architecture,, run the following command 1 python -m examples.sentence_level.wmt_2018.en_cs.monotransquest Results Both architectures in TransQuest outperforms QuEst++ in all the language pairs. Language Pair Algorithm Pearson MAE RMSE English-German (NMT) MonoTransQuest 0.4784 0.1264 0.1770 SiameseTransQuest 0.4152 0.1270 0.1796 QuEst++ 0.2874 0.1286 0.1886 English-German (SMT) MonoTransQuest 0.7355 0.0967 0.1300 SiameseTransQuest 0.6992 0.1258 0.1438 QuEst++ 0.3653 0.1402 0.1772 English-Latvian (NMT) MonoTransQuest 0.7450 0.1162 0.1601 SiameseTransQuest 0.7183 0.1456 0.1892 QuEst++ 0.4435 0.1625 0.2164 English-Latvian (SMT) MonoTransQuest 0.7141 0.1041 0.1420 SiameseTransQuest 0.6320 0.1274 0.1661 QuEst++ 0.3528 0.1554 0.1919 English-Czech MonoTransQuest 0.7207 0.1197 0.1631 SiameseTransQuest 0.6853 0.1298 0.1801 QuEst++ 0.3943 0.1651 0.2110 German-English MonoTransQuest 0.7939 0.0934 0.1277 SiameseTransQuest 0.7524 0.1194 0.1502 QuEst++ 0.3323 0.1508 0.1928 Tip Too tired to train QE models? Checkout our model zoo.","title":"Sentence-level"},{"location":"examples/sentence_level_examples/#sentence-level-examples","text":"We have provided several examples on how to use TransQuest in recent WMT sentence-level quality estimation shared tasks. They are included in the repository but are not shipped with the library. Therefore, if you need to run the examples, please clone the repository. Warning Please don't use the same environment you used to install TransQuest to run the examples. Create a new environment. 1 2 3 git clone https://github.com/TharinduDR/TransQuest.git cd TransQuest pip install -r requirements.txt In the examples/sentence_level folder you will find the following tasks.","title":"Sentence Level Examples"},{"location":"examples/sentence_level_examples/#wmt-2020-qe-task-1-sentence-level-direct-assessment","text":"The participants were predict the direct assessment of a source and a target. There were seven language-pairs released by the organisers. To run the experiments for each language please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2020.<language-pair>.<architecture> Language Pair options : ro_en (Romanian-English), ru_en (Russian-English), et_en (Estonian-English), en_zh (English-Chinese), ne_en (Nepalese-English), en_de (English-German), si_en(Sinhala-English) Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on Romanian-English with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2020.ro_en.monotransquest","title":"WMT 2020 QE Task 1: Sentence-Level Direct Assessment"},{"location":"examples/sentence_level_examples/#results","text":"Both architectures in TransQuest outperforms OpenKiwi in all the language pairs. Furthermore, TransQuest won this task in all the language pairs. Language Pair Algorithm Pearson MAE RMSE Romanian-English MonoTransQuest 0.8982 0.3121 0.4097 SiameseTransQuest 0.8501 0.3637 0.4932 OpenKiwi 0.6845 0.7596 1.0522 Estonian-English MonoTransQuest 0.7748 0.5904 0.7321 SiameseTransQuest 0.6804 0.7047 0.9022 OpenKiwi 0.4770 0.9176 1.1382 Nepalese-English MonoTransQuest 0.7914 0.3975 0.5078 SiameseTransQuest 0.6081 0.6531 0.7950 OpenKiwi 0.3860 0.7353 0.8713 Sinhala-English MonoTransQuest 0.6525 0.4510 0.5570 SiameseTransQuest 0.5957 0.5078 0.6466 OpenKiwi 0.3737 0.7517 0.8978 Russian-English MonoTransQuest 0.7734 0.5076 0.6856 SiameseTransQuest 0.7126 0.6132 0.8531 OpenKiwi 0.5479 0.8253 1.1930 English-German MonoTransQuest 0.4669 0.6474 0.7762 SiameseTransQuest 0.3992 0.6651 0.8497 OpenKiwi 0.1455 0.6791 0.9670 English-Chinese MonoTransQuest 0.4779 0.9865 1.1338 SiameseTransQuest 0.4067 1.0389 1.1973 OpenKiwi 0.1676 0.6559 0.8503","title":"Results"},{"location":"examples/sentence_level_examples/#wmt-2020-qe-task-2-sentence-level-post-editing-effort","text":"This task consists predicting Sentence-level HTER (Human Translation Error Rate) scores for a given source and a target. To run the experiments for each language please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2020_task2.<language-pair>.<architecture> Language Pair options : en_zh (English-Chinese), en_de (English-German) Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on English-Chinese with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2020_task2.en_zh.monotransquest","title":"WMT 2020 QE Task 2: Sentence-Level Post-editing Effort"},{"location":"examples/sentence_level_examples/#results_1","text":"Both architectures in TransQuest outperforms OpenKiwi in all the language pairs. Language Pair Algorithm Pearson MAE RMSE English-German MonoTransQuest 0.4994 0.1486 0.1842 SiameseTransQuest 0.4875 0.1501 0.1886 OpenKiwi 0.3916 0.1500 0.1896 English-Chinese MonoTransQuest 0.5910 0.1351 0.1681 SiameseTransQuest 0.5621 0.1411 0.1723 OpenKiwi 0.5058 0.1470 0.1814","title":"Results"},{"location":"examples/sentence_level_examples/#wmt-2019-qe-task-1-sentence-level-qe","text":"The participating systems are expected to predict the sentence-level HTER score (the percentage of edits needed to fix the translation) To run the experiments for each language, please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2019.<language-pair>.<architecture> Language Pair options : en_ru (English-Russian), en_de (English-German) Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on English-Russian with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2019.en_ru.monotransquest","title":"WMT 2019 QE Task 1: Sentence-Level QE"},{"location":"examples/sentence_level_examples/#results_2","text":"Both architectures in TransQuest outperforms QuEst++ in all the language pairs. Language Pair Algorithm Pearson English-German MonoTransQuest 0.5117 SiameseTransQuest 0.4951 QuEst++ 0.4001 English-Russian MonoTransQuest 0.7126 SiameseTransQuest 0.6432 QuEst++ 0.2601","title":"Results"},{"location":"examples/sentence_level_examples/#wmt-2018-qe-task-1-sentence-level-qe","text":"The participating systems are expected to predict the sentence-level HTER score (the percentage of edits needed to fix the translation) To run the experiments for each language, please run this command from the root directory of TransQuest. If both NMT and SMT is available for a certain language pair, specify that too. 1 python -m examples.sentence_level.wmt_2019.<language-pair>.<nmt/smt><architecture> Language Pair options : en_de (English-German) (both NMT and SMT), en_lv(English-Latvian) (both NMT and SMT), en_cs(English-Czech), de_en Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest). As an example to run the experiments on English-Latvian NMT with MonoTransQuest architecture, run the following command. 1 python -m examples.sentence_level.wmt_2018.en_lv.nmt.monotransquest To run the English-Czech experiments with MonoTransQuest architecture,, run the following command 1 python -m examples.sentence_level.wmt_2018.en_cs.monotransquest","title":"WMT 2018 QE Task 1: Sentence-Level QE"},{"location":"examples/sentence_level_examples/#results_3","text":"Both architectures in TransQuest outperforms QuEst++ in all the language pairs. Language Pair Algorithm Pearson MAE RMSE English-German (NMT) MonoTransQuest 0.4784 0.1264 0.1770 SiameseTransQuest 0.4152 0.1270 0.1796 QuEst++ 0.2874 0.1286 0.1886 English-German (SMT) MonoTransQuest 0.7355 0.0967 0.1300 SiameseTransQuest 0.6992 0.1258 0.1438 QuEst++ 0.3653 0.1402 0.1772 English-Latvian (NMT) MonoTransQuest 0.7450 0.1162 0.1601 SiameseTransQuest 0.7183 0.1456 0.1892 QuEst++ 0.4435 0.1625 0.2164 English-Latvian (SMT) MonoTransQuest 0.7141 0.1041 0.1420 SiameseTransQuest 0.6320 0.1274 0.1661 QuEst++ 0.3528 0.1554 0.1919 English-Czech MonoTransQuest 0.7207 0.1197 0.1631 SiameseTransQuest 0.6853 0.1298 0.1801 QuEst++ 0.3943 0.1651 0.2110 German-English MonoTransQuest 0.7939 0.0934 0.1277 SiameseTransQuest 0.7524 0.1194 0.1502 QuEst++ 0.3323 0.1508 0.1928 Tip Too tired to train QE models? Checkout our model zoo.","title":"Results"},{"location":"examples/word_level_examples/","text":"Word Level Examples We have provided several examples on how to use TransQuest in recent WMT word-level quality estimation shared tasks. They are included in the repository but are not shipped with the library. Therefore, if you need to run the examples, please clone the repository. Warning Please don't use the same environment you used to install TransQuest to run the examples. Create a new environment. 1 2 3 git clone https://github.com/TharinduDR/TransQuest.git cd TransQuest pip install -r requirements.txt In the examples/word_level folder you will find the following tasks. WMT 2020 QE Task 2: Word-Level Post-editing Effort This task consists predicting Word-level quality for a given source and a target. It requires predicting word level quality in source and target as OK, BAD and also the quality of the \"gaps\" in target as Ok, BAD. To run the experiments for each language please run this command from the root directory of TransQuest. 1 python -m examples.word_level.wmt_2020.<language-pair>.<architecture> Language Pair options : en_zh (English-Chinese), en_de (English-German) Architecture Options : microtransquest (MicroTransQuest) As an example to run the experiments on English-Chinese with MicroTransQuwst architecture, run the following command. 1 python -m examples.word_level.wmt_2020.en_zh.microtransquest Results MicroTransQuest architecture in TransQuest outperforms OpenKiwi in all the language pairs. Language Pair Algorithm Source F1 Multi Target F1 Multi English-German MicroTransQuest 0.5456 0.6013 OpenKiwi 0.3717 0.4111 English-Chinese MicroTransQuest 0.4440 0.6402 OpenKiwi 0.3729 0.5583 WMT 2019 QE Task 2: Word-Level QE The participating systems are expected to predict the Word-level quality for a given source and a target. To run the experiments for each language, please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2019.<language-pair>.<architecture> Language Pair options : en_ru (English-Russian) Architecture Options : microtransquest (MicroTransQuest) As an example to run the experiments on English-Russian with MicroTransQuest architecture, run the following command. 1 python -m examples.word_level.wmt_2019.en_ru.microtransquest Results MicroTransQuest architecture in TransQuest outperforms OpenKiwi in En-Ru. Language Pair Algorithm Source F1 Multi Target F1 Multi English-Russian MicroTransQuest 0.5543 0.5592 OpenKiwi 0.2647 0.2412 WMT 2018 QE Task 2: Word-Level QE The participating systems are expected to predict the Word-level quality for a given source and a target. To run the experiments for each language, please run this command from the root directory of TransQuest. If both NMT and SMT is available for a certain language pair, specify that too. 1 python -m examples.word_level.wmt_2019.<language-pair>.<nmt/smt><architecture> Language Pair options : en_de (English-German) (both NMT and SMT), en_lv(English-Latvian) (both NMT and SMT), en_cs(English-Czech), de_en Architecture Options : microtransquest (MicroTransQuest) As an example to run the experiments on English-Latvian NMT with MicroTransQuest architecture, run the following command. 1 python -m examples.word_level.wmt_2018.en_lv.nmt.microtransquest To run the English-Czech experiments with MicroTransQuest architecture,, run the following command 1 python -m examples.word_level.wmt_2018.en_cs.microtransquest Results MicroTransQuest architecture in TransQuest outperforms Marmot in all the language pairs. Language Pair Algorithm Source F1 Multi Target F1 Multi Gaps F1 Multi English-German (NMT) MicroTransQuest 0.2957 0.4421 0.1672 Marmot 0.0000 0.1812 0.0000 English-German (SMT) MicroTransQuest 0.5269 0.6348 0.4927 Marmot 0.0000 0.3630 0.0000 English-Latvian (NMT) MicroTransQuest 0.4880 0.5868 0.1664 Marmot 0.0000 0.4208 0.0000 English-Latvian (SMT) MicroTransQuest 0.4945 0.5939 0.2356 Marmot 0.0000 0.3445 0.0000 English-Czech MicroTransQuest 0.5327 0.6081 0.2018 Marmot 0.0000 0.4449 0.0000 German-English MicroTransQuest 0.4824 0.6485 0.4203 Marmot 0.0000 0.4373 0.0000 Note Please note that in WMT 2018 the organisers evaluated the gaps and the words in MT separately. This is different from WMT 2019 and WMT 2020. Note Please note that the baseline used in WMT 2018; Marmot does not support predicting quality for words in source and gaps in target. Hence, those values are set to 0.0000 in all the language pairs. Tip Too tired to train QE models? Checkout our model zoo.","title":"Word-level"},{"location":"examples/word_level_examples/#word-level-examples","text":"We have provided several examples on how to use TransQuest in recent WMT word-level quality estimation shared tasks. They are included in the repository but are not shipped with the library. Therefore, if you need to run the examples, please clone the repository. Warning Please don't use the same environment you used to install TransQuest to run the examples. Create a new environment. 1 2 3 git clone https://github.com/TharinduDR/TransQuest.git cd TransQuest pip install -r requirements.txt In the examples/word_level folder you will find the following tasks.","title":"Word Level Examples"},{"location":"examples/word_level_examples/#wmt-2020-qe-task-2-word-level-post-editing-effort","text":"This task consists predicting Word-level quality for a given source and a target. It requires predicting word level quality in source and target as OK, BAD and also the quality of the \"gaps\" in target as Ok, BAD. To run the experiments for each language please run this command from the root directory of TransQuest. 1 python -m examples.word_level.wmt_2020.<language-pair>.<architecture> Language Pair options : en_zh (English-Chinese), en_de (English-German) Architecture Options : microtransquest (MicroTransQuest) As an example to run the experiments on English-Chinese with MicroTransQuwst architecture, run the following command. 1 python -m examples.word_level.wmt_2020.en_zh.microtransquest","title":"WMT 2020 QE Task 2: Word-Level Post-editing Effort"},{"location":"examples/word_level_examples/#results","text":"MicroTransQuest architecture in TransQuest outperforms OpenKiwi in all the language pairs. Language Pair Algorithm Source F1 Multi Target F1 Multi English-German MicroTransQuest 0.5456 0.6013 OpenKiwi 0.3717 0.4111 English-Chinese MicroTransQuest 0.4440 0.6402 OpenKiwi 0.3729 0.5583","title":"Results"},{"location":"examples/word_level_examples/#wmt-2019-qe-task-2-word-level-qe","text":"The participating systems are expected to predict the Word-level quality for a given source and a target. To run the experiments for each language, please run this command from the root directory of TransQuest. 1 python -m examples.sentence_level.wmt_2019.<language-pair>.<architecture> Language Pair options : en_ru (English-Russian) Architecture Options : microtransquest (MicroTransQuest) As an example to run the experiments on English-Russian with MicroTransQuest architecture, run the following command. 1 python -m examples.word_level.wmt_2019.en_ru.microtransquest","title":"WMT 2019 QE Task 2: Word-Level QE"},{"location":"examples/word_level_examples/#results_1","text":"MicroTransQuest architecture in TransQuest outperforms OpenKiwi in En-Ru. Language Pair Algorithm Source F1 Multi Target F1 Multi English-Russian MicroTransQuest 0.5543 0.5592 OpenKiwi 0.2647 0.2412","title":"Results"},{"location":"examples/word_level_examples/#wmt-2018-qe-task-2-word-level-qe","text":"The participating systems are expected to predict the Word-level quality for a given source and a target. To run the experiments for each language, please run this command from the root directory of TransQuest. If both NMT and SMT is available for a certain language pair, specify that too. 1 python -m examples.word_level.wmt_2019.<language-pair>.<nmt/smt><architecture> Language Pair options : en_de (English-German) (both NMT and SMT), en_lv(English-Latvian) (both NMT and SMT), en_cs(English-Czech), de_en Architecture Options : microtransquest (MicroTransQuest) As an example to run the experiments on English-Latvian NMT with MicroTransQuest architecture, run the following command. 1 python -m examples.word_level.wmt_2018.en_lv.nmt.microtransquest To run the English-Czech experiments with MicroTransQuest architecture,, run the following command 1 python -m examples.word_level.wmt_2018.en_cs.microtransquest","title":"WMT 2018 QE Task 2: Word-Level QE"},{"location":"examples/word_level_examples/#results_2","text":"MicroTransQuest architecture in TransQuest outperforms Marmot in all the language pairs. Language Pair Algorithm Source F1 Multi Target F1 Multi Gaps F1 Multi English-German (NMT) MicroTransQuest 0.2957 0.4421 0.1672 Marmot 0.0000 0.1812 0.0000 English-German (SMT) MicroTransQuest 0.5269 0.6348 0.4927 Marmot 0.0000 0.3630 0.0000 English-Latvian (NMT) MicroTransQuest 0.4880 0.5868 0.1664 Marmot 0.0000 0.4208 0.0000 English-Latvian (SMT) MicroTransQuest 0.4945 0.5939 0.2356 Marmot 0.0000 0.3445 0.0000 English-Czech MicroTransQuest 0.5327 0.6081 0.2018 Marmot 0.0000 0.4449 0.0000 German-English MicroTransQuest 0.4824 0.6485 0.4203 Marmot 0.0000 0.4373 0.0000 Note Please note that in WMT 2018 the organisers evaluated the gaps and the words in MT separately. This is different from WMT 2019 and WMT 2020. Note Please note that the baseline used in WMT 2018; Marmot does not support predicting quality for words in source and gaps in target. Hence, those values are set to 0.0000 in all the language pairs. Tip Too tired to train QE models? Checkout our model zoo.","title":"Results"},{"location":"models/sentence_level_pretrained/","text":"Sentence Level Pre-trained Models We have released several pre-trained TransQuest models on two aspects in sentence-level quality estimation. We will be keep releasing new models. So please keep in touch. Predicting Direct Assessment The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality, where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics We have released several quality estimation models for this aspect. We have also released a couple of multi-language pair models that would work on any language pair in any domain. Available Models Language Pair NMT/SMT Domain Algorithm Model Link Romanian-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Estonian-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Nepalese-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Sinhala-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Russian-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip English-German NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip English-Chinese NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip English-* Any Any MonoTransQuest model.zip SiameseTransQuest *-English Any Any MonoTransQuest model.zip SiameseTransQuest *-* Any Any MonoTransQuest model.zip SiameseTransQuest Note * denotes any language. (*-* means any language to any language) Predicting HTER The performance of QE systems has typically been assessed using the semiautomatic HTER (Human-mediated Translation Edit Rate). HTER is an edit-distance-based measure which captures the distance between the automatic translation and a reference translation in terms of the number of modifications required to transform one into another. In light of this, a QE system should be able to predict the percentage of edits required in the translation. We have released several quality estimation models for this aspect. We have also released a couple of multi-language pair models that would work on any language pair in any domain. Available Models Language Pair NMT/SMT Domain Algorithm Model Link English-German NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip SMT IT MonoTransQuest model.zip SiameseTransQuest English-Latvian SMT Life Sciences MonoTransQuest model.zip SiameseTransQuest English-Latvian NMT Life Sciences MonoTransQuest model.zip SiameseTransQuest English-Czech SMT IT MonoTransQuest model.zip SiameseTransQuest German-English SMT Life Sciences MonoTransQuest model.zip SiameseTransQuest English-Chinese NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest English-* Any Any MonoTransQuest SiameseTransQuest *-* Any Any MonoTransQuest SiameseTransQuest Note * denotes any language. (*-* means any language to any language)","title":"Sentence-level"},{"location":"models/sentence_level_pretrained/#sentence-level-pre-trained-models","text":"We have released several pre-trained TransQuest models on two aspects in sentence-level quality estimation. We will be keep releasing new models. So please keep in touch.","title":"Sentence Level Pre-trained Models"},{"location":"models/sentence_level_pretrained/#predicting-direct-assessment","text":"The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality, where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics We have released several quality estimation models for this aspect. We have also released a couple of multi-language pair models that would work on any language pair in any domain.","title":"Predicting Direct Assessment"},{"location":"models/sentence_level_pretrained/#available-models","text":"Language Pair NMT/SMT Domain Algorithm Model Link Romanian-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Estonian-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Nepalese-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Sinhala-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip Russian-English NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip English-German NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip English-Chinese NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip English-* Any Any MonoTransQuest model.zip SiameseTransQuest *-English Any Any MonoTransQuest model.zip SiameseTransQuest *-* Any Any MonoTransQuest model.zip SiameseTransQuest Note * denotes any language. (*-* means any language to any language)","title":"Available Models"},{"location":"models/sentence_level_pretrained/#predicting-hter","text":"The performance of QE systems has typically been assessed using the semiautomatic HTER (Human-mediated Translation Edit Rate). HTER is an edit-distance-based measure which captures the distance between the automatic translation and a reference translation in terms of the number of modifications required to transform one into another. In light of this, a QE system should be able to predict the percentage of edits required in the translation. We have released several quality estimation models for this aspect. We have also released a couple of multi-language pair models that would work on any language pair in any domain.","title":"Predicting HTER"},{"location":"models/sentence_level_pretrained/#available-models_1","text":"Language Pair NMT/SMT Domain Algorithm Model Link English-German NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest model.zip SMT IT MonoTransQuest model.zip SiameseTransQuest English-Latvian SMT Life Sciences MonoTransQuest model.zip SiameseTransQuest English-Latvian NMT Life Sciences MonoTransQuest model.zip SiameseTransQuest English-Czech SMT IT MonoTransQuest model.zip SiameseTransQuest German-English SMT Life Sciences MonoTransQuest model.zip SiameseTransQuest English-Chinese NMT Wikipedia MonoTransQuest model.zip SiameseTransQuest English-* Any Any MonoTransQuest SiameseTransQuest *-* Any Any MonoTransQuest SiameseTransQuest Note * denotes any language. (*-* means any language to any language)","title":"Available Models"},{"location":"models/word_level_pretrained/","text":"Word Level Pre-trained Models We have released several pre-trained TransQuest models on word-level quality estimation. We will be keep releasing new models. So please keep in touch. Available Models Language Pair NMT/SMT Domain Algorithm Model Link English-German NMT IT MicroTransQuest model.zip NMT Wiki MicroTransQuest SMT IT MicroTransQuest model.zip English-Latvian NMT Life Sciences MicroTransQuest model.zip SMT Life Sciences MicroTransQuest model.zip English-Czech SMT IT MicroTransQuest model.zip German-English SMT Life Sciences MicroTransQuest model.zip English-Chinese NMT Wikipedia MicroTransQuest English-Russian NMT IT MicroTransQuest *-* Any Any MicroTransQuest Note * denotes any language. (*-* means any language to any language)","title":"Word-level"},{"location":"models/word_level_pretrained/#word-level-pre-trained-models","text":"We have released several pre-trained TransQuest models on word-level quality estimation. We will be keep releasing new models. So please keep in touch.","title":"Word Level Pre-trained Models"},{"location":"models/word_level_pretrained/#available-models","text":"Language Pair NMT/SMT Domain Algorithm Model Link English-German NMT IT MicroTransQuest model.zip NMT Wiki MicroTransQuest SMT IT MicroTransQuest model.zip English-Latvian NMT Life Sciences MicroTransQuest model.zip SMT Life Sciences MicroTransQuest model.zip English-Czech SMT IT MicroTransQuest model.zip German-English SMT Life Sciences MicroTransQuest model.zip English-Chinese NMT Wikipedia MicroTransQuest English-Russian NMT IT MicroTransQuest *-* Any Any MicroTransQuest Note * denotes any language. (*-* means any language to any language)","title":"Available Models"}]}